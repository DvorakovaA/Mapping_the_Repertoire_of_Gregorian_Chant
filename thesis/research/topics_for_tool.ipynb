{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic models for tool  \n",
    "\n",
    " Here is code that creates topic models to be used for computing communities inside tool.  \n",
    " Five files are needed:\n",
    "- topic_2.model -> smallest_count_vec\n",
    "- topic_5.model -> less_count_vec\n",
    "- topic_10.model -> less_count_vec\n",
    "- topic_20.model -> less_count_vec\n",
    "- topic_reduction.model (copy of topic_20.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lzma\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "responsories_all = pd.read_csv('../data/all-ci-responsories.csv', usecols=['cantus_id', 'incipit', 'siglum', 'source_id', 'feast_id'], dtype={'cantus_id':\"str\"})\n",
    "antiphons_all = pd.read_csv('../data/all-ci-antiphons.csv', usecols=['cantus_id', 'incipit', 'siglum', 'source_id', 'feast_id'], dtype={'cantus_id':\"str\"})\n",
    "\n",
    "sources = pd.read_csv('../data/sources-with-provenance-ids-and-two-centuries.csv', usecols=['provenance_id', 'drupal_path', 'siglum', 'cursus', 'num_century'])\n",
    "feasts = pd.read_csv('../data/feast.csv', usecols=['id', 'name'])\n",
    "\n",
    "chants = pd.concat([responsories_all, antiphons_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dict to index sources\n",
    "source_dict = OrderedDict()\n",
    "i = 0\n",
    "for id in sources['drupal_path']:\n",
    "    source_dict[id] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chants in our sources 362632\n",
      "Number of CIDs used in our sources 17599\n"
     ]
    }
   ],
   "source": [
    "# Transform chant data into document like structure\n",
    "source_all_chants_dict = {}\n",
    "used_cantus_ids = []\n",
    "for source_id in sources['drupal_path'].tolist():\n",
    "    filt_source = chants['source_id'] == source_id\n",
    "    used_cantus_ids += (chants[filt_source]['cantus_id']).tolist()\n",
    "    source_all_chants_dict[source_id] = ' '.join((chants[filt_source]['cantus_id']).tolist())\n",
    "\n",
    "print(\"Number of chants in our sources\", len(used_cantus_ids))\n",
    "freq_CIDs = Counter(used_cantus_ids)\n",
    "print('Number of CIDs used in our sources', len(set(used_cantus_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most wide [sources x chants] matrix shape: (250, 17599)\n",
      "Loosing 0 CIDs\n"
     ]
    }
   ],
   "source": [
    "# Construct [sources x chants] matrix (document word matrix) for almost all data\n",
    "all_count_vec = CountVectorizer(max_df=len(sources), min_df=0.0, token_pattern='\\\\b(\\\\w+[\\\\.:]?\\\\w+)\\\\b')\n",
    "all_count_vec_data = all_count_vec.fit_transform(source_all_chants_dict.values())\n",
    "print(\"Most wide [sources x chants] matrix shape:\", all_count_vec_data.shape)\n",
    "print(\"Loosing\", len(set(used_cantus_ids)) - all_count_vec_data.shape[1], \"CIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less wide [sources x chants] matrix shape: (250, 10368)\n",
      "Loosing 7231 CIDs\n"
     ]
    }
   ],
   "source": [
    "# Construct [sources x chants] matrix (document word matrix) for choosen data\n",
    "# we use only words which are in 250 or less docs and also it at least two docs\n",
    "less_count_vec = CountVectorizer(max_df=250, min_df=2, token_pattern='\\\\b(\\\\w+[\\\\.:]?\\\\w+)\\\\b')\n",
    "less_count_vec_data = less_count_vec.fit_transform(source_all_chants_dict.values())\n",
    "print(\"Less wide [sources x chants] matrix shape:\", less_count_vec_data.shape)\n",
    "print(\"Loosing\", len(set(used_cantus_ids)) - less_count_vec_data.shape[1], \"CIDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least wide [sources x chants] matrix shape: (250, 4924)\n",
      "Loosing 12675 CIDs\n"
     ]
    }
   ],
   "source": [
    "# Construct [sources x chants] matrix (document word matrix) for choosen data\n",
    "# we use only words which are in 250 or less docs and also it at least eight docs\n",
    "smallest_count_vec = CountVectorizer(max_df=250, min_df=8, token_pattern='\\\\b(\\\\w+[\\\\.:]?\\\\w+)\\\\b')\n",
    "smallest_count_vec_data = smallest_count_vec.fit_transform(source_all_chants_dict.values())\n",
    "print(\"The least wide [sources x chants] matrix shape:\", smallest_count_vec_data.shape)\n",
    "print(\"Loosing\", len(set(used_cantus_ids)) - smallest_count_vec_data.shape[1], \"CIDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Model for dimension reduction and further counting distance between chants sets\n",
    "# uses all_count_vec, max_iter = 40, evaluate_every = 1, 20 topics\n",
    "random_state_red = [i for i in range(1, 21)]\n",
    "random.seed(42)\n",
    "compare_sources = random.sample(sources['drupal_path'].tolist(), 20)\n",
    "\n",
    "perplexities_20 = {}\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    model = LatentDirichletAllocation(n_components=20, evaluate_every=1, max_iter=40, random_state=random_state_red[i])\n",
    "    model.fit(less_count_vec_data)\n",
    "    compare_data = less_count_vec.transform([source_all_chants_dict[s] for s in compare_sources])\n",
    "    perplexities_20[i] = model.perplexity(compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31769.229719598163\n",
      "{0: 39828.7980955025, 1: 36856.97205927947, 2: 33994.29272628802, 3: 33598.63686882714, 4: 44800.10066494636, 5: 38153.11834538742, 6: 31853.28652568756, 7: 31769.229719598163, 8: 41968.79895148937, 9: 34329.50208995581}\n"
     ]
    }
   ],
   "source": [
    "print(min(perplexities_20.values()))\n",
    "print(perplexities_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20 = LatentDirichletAllocation(n_components=20, verbose=1, evaluate_every=1, max_iter=40, random_state=random_state_red[7])\n",
    "model_20.fit(less_count_vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with lzma.open('topic_20.model', \"wb\") as model_file:\n",
    "#    pickle.dump(all_count_vec, model_file)\n",
    "#    pickle.dump(model_20, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with lzma.open('topic_reduction.model', \"wb\") as model_file:\n",
    "#    pickle.dump(all_count_vec, model_file)\n",
    "#    pickle.dump(model_20, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_2 = [i for i in range(1, 21)]\n",
    "random.seed(42)\n",
    "compare_sources = random.sample(sources['drupal_path'].tolist(), 20)\n",
    "\n",
    "perplexities_2 = {}\n",
    "for i in range(10):\n",
    "    model = LatentDirichletAllocation(n_components=2, verbose=1, evaluate_every=1, max_iter=40, random_state=random_state_2[i])\n",
    "    model.fit(smallest_count_vec_data)\n",
    "    compare_data = smallest_count_vec.transform([source_all_chants_dict[s] for s in compare_sources])\n",
    "    perplexities_2[i] = model.perplexity(compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5236.587591633831\n",
      "{0: 5243.6288023577345, 1: 5237.336296994617, 2: 5236.587591633831, 3: 5244.458623793637, 4: 5243.340633690007, 5: 5236.703858324916, 6: 5242.975047236425, 7: 5243.9313452122715, 8: 5243.016821268301, 9: 5243.149331856894}\n"
     ]
    }
   ],
   "source": [
    "print(min(perplexities_2.values()))\n",
    "print(perplexities_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = LatentDirichletAllocation(n_components=2, verbose=1, evaluate_every=1, max_iter=40, random_state=random_state_2[2])\n",
    "model_2.fit(smallest_count_vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with lzma.open('topic_2.model', \"wb\") as model_file:\n",
    "#    pickle.dump(smallest_count_vec, model_file)\n",
    "#    pickle.dump(model_2, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_5 = [i for i in range(1, 21)]\n",
    "random.seed(42)\n",
    "compare_sources = random.sample(sources['drupal_path'].tolist(), 20)\n",
    "\n",
    "perplexities_5 = {}\n",
    "for i in range(10):\n",
    "    model = LatentDirichletAllocation(n_components=5, verbose=1, evaluate_every=1, max_iter=40, random_state=random_state_5[i])\n",
    "    model.fit(less_count_vec_data)\n",
    "    compare_data = smallest_count_vec.transform([source_all_chants_dict[s] for s in compare_sources])\n",
    "    perplexities_5[i] = model.perplexity(compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51478.15555992069\n",
      "{0: 51478.15555992069, 1: 52225.4696988275, 2: 58524.881422371705, 3: 57762.97239178083, 4: 54959.91432748364, 5: 61054.94187125619, 6: 52872.45259469045, 7: 58771.73939111505, 8: 56512.79218479323, 9: 58007.35859670519}\n"
     ]
    }
   ],
   "source": [
    "print(min(perplexities_5.values()))\n",
    "print(perplexities_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = LatentDirichletAllocation(verbose=1, n_components=5, max_iter=40, evaluate_every=1, random_state=random_state_5[0])\n",
    "model_5.fit(smallest_count_vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with lzma.open('topic_5.model', \"wb\") as model_file:\n",
    "#    pickle.dump(smallest_count_vec, model_file)\n",
    "#    pickle.dump(model_5, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_10 = [i for i in range(1, 21)]\n",
    "random.seed(42)\n",
    "compare_sources = random.sample(sources['drupal_path'].tolist(), 20)\n",
    "\n",
    "perplexities_10 = {}\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    model = LatentDirichletAllocation(n_components=10, evaluate_every=1, max_iter=40, random_state=random_state_10[i])\n",
    "    model.fit(less_count_vec_data)\n",
    "    compare_data = less_count_vec.transform([source_all_chants_dict[s] for s in compare_sources])\n",
    "    perplexities_10[i] = model.perplexity(compare_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19283.908158166683\n",
      "{0: 22221.28680063228, 1: 20350.356529784898, 2: 21678.813526315622, 3: 23243.557698737804, 4: 23610.793130490492, 5: 23379.85579399877, 6: 19978.44829975222, 7: 19487.23741825979, 8: 19283.908158166683, 9: 23271.235943281205}\n"
     ]
    }
   ],
   "source": [
    "print(min(perplexities_10.values()))\n",
    "print(perplexities_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10 = LatentDirichletAllocation(verbose=1, n_components=10, max_iter=40, evaluate_every=1, random_state=random_state_10[8])\n",
    "model_10.fit(less_count_vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with lzma.open('topic_10.model', \"wb\") as model_file:\n",
    "#    pickle.dump(all_count_vec, model_file)\n",
    "#    pickle.dump(model_10, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
